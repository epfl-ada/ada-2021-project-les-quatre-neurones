{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "This notebook allows you to filter the original datasets in order to keep only what you need for the project. The second part is dedicated to the QIDS associated to Wikidata in order to find the labels of each of them. These labels will be useful to interpret the results at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import urllib.request as r\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen, Request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark context\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Filtering the dataset \n",
    "This part 'filter' the original datasets by year in order to take only the date, quoteID, the quotation and the QIDS of the speaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2020.json.bz2' \n",
    "path_to_out = 'data/sp_qids_quotes-2020-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2019.json.bz2' \n",
    "path_to_out = 'data/quotes-2019-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2018.json.bz2' \n",
    "path_to_out = 'data/quotes-2018-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2017.json.bz2' \n",
    "path_to_out = 'data/quotes-2017-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2016.json.bz2' \n",
    "path_to_out = 'data/quotes-2016-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/quotes-2015.json.bz2' \n",
    "path_to_out = 'data/quotes-2015-process.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #instance['date'] = datetime.strptime(instance['date'], '%Y-%m-%d %H:%M:%S') \n",
    "            instance['quoteID']= instance['quoteID'][11:]\n",
    "            if(len(instance['qids'])!=0):\n",
    "                instance['qids'] = instance['qids'][0]\n",
    "                instance = dict((key,value) for key, value in instance.items() if key in('date','quoteID','quotation','qids'))\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Speaker attributes\n",
    "This part allows you to take all the QIDS that may be useful and find their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data against speaker attributes. Rename also the columns to keep a certain coherence in the following processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = pd.read_parquet('data/speaker_attributes.parquet')\n",
    "df_sp = df_sp[['id','label','nationality','occupation','ethnic_group', 'party','religion']].copy()\n",
    "df_sp = df_sp.rename(columns={\"nationality\": \"nationality_qids\", \"ethnic_group\": \"ethnic_qids\",\"occupation\":\"occupation_qids\",\"party\":'party_qids','id':'speaker_qids','label':'speaker_label','religion':'religion_qids'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all the QIDS found in the atrributes of the speakers. Each column in the speaker dataframe has its own QIDS list and the labels are also retrieved column by column. A dictionary is then created for each column with the QIDS and the corresponding label. Some urls did not work (Error 404 example) and they are put in comment each time. \n",
    "There were more than 14'000 occupations, we had to separate the list into several sub-lists in order to find the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_qids = df_sp['nationality_qids'].explode('nationality_qids').drop_duplicates().values\n",
    "occupation_qids = df_sp['occupation_qids'].explode('occupation_qids').drop_duplicates().values\n",
    "ethnic_qids = df_sp['ethnic_qids'].explode('ethnic_qids').drop_duplicates().values\n",
    "party_qids = df_sp['party_qids'].explode('party_qids').drop_duplicates().values\n",
    "religion_qids = df_sp['religion_qids'].explode('religion_qids').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label(qids):\n",
    "    \"\"\"Use Beautiful soup parser in order to find for each QIDS their label on their Wikidata's page\"\"\"\n",
    "    url = 'https://www.wikidata.org/wiki/'\n",
    "    url_qids = url+qids\n",
    "    ru =r.Request(url_qids,headers={'Connection': 'close'})\n",
    "    page = urlopen(ru).read()\n",
    "    soup = BeautifulSoup(page)\n",
    "    label = soup.find(\"span\", {\"class\":\"wikibase-title-label\"}).text\n",
    "    return label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Religion QIDS - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_qids = [x for x in religion_qids if x!=None]\n",
    "df_religion = pd.DataFrame()\n",
    "df_religion['qids'] = religion_qids\n",
    "df_religion['label'] = df_religion['qids'].apply(lambda x : find_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_religion.to_json('data/religion.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nationality QIDS - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_qids = [x for x in nationality_qids if x!=None]\n",
    "df_nationality = pd.DataFrame()\n",
    "df_nationality['qids'] = nationality_qids\n",
    "df_nationality['label'] = df_nationality['qids'].apply(lambda x : find_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nationality.to_json('data/nationality.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnic QIDS - label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic_qids = [x for x in ethnic_qids if x!=None]\n",
    "df_ethnic = pd.DataFrame()\n",
    "df_ethnic['ethnic_qids'] = ethnic_qids\n",
    "df_ethnic['ethnic_label'] = df_ethnic['ethnic_qids'].apply(lambda x : find_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethnic.to_json('data/ethnic.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Party QIDS - label \n",
    "- n° of party: 9'632\n",
    "- Problem: Q99761286 - <Response [404]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_qids = [x for x in party_qids if x!=None]\n",
    "df_party = pd.DataFrame()\n",
    "df_party['party_qids'] = party_qids\n",
    "df_party = df_party[df_party.party_qids != 'Q99761286']\n",
    "df_party['party_label'] = df_party['party_qids'].apply(lambda x : find_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_party.to_json('data/party.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Occupation QIDS - label\n",
    "- Need to split in 3 arrays in order to be more efficient\n",
    "- 'Q57557390', 'Q102046591', 'Q98384826', 'Q105645755', 'Q99753484','Q56411328', 'Q96143085', 'Q96144081', 'Q3738699', 'Q5568256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_qids = [x for x in occupation_qids if x!=None]\n",
    "l = len(occupation_qids)\n",
    "l1 = 500\n",
    "l2 = 1000\n",
    "occupation_qids_1 = occupation_qids[:l1]\n",
    "occupation_qids_2 = occupation_qids[l1:l2]\n",
    "occupation_qids_3 = occupation_qids[1000:5000]\n",
    "occupation_qids_4 = occupation_qids[5000:10000]\n",
    "occupation_qids_5 = occupation_qids[10000:]\n",
    "a = ['Q57557390', 'Q102046591', 'Q98384826', 'Q105645755', 'Q99753484','Q56411328','Q96143085', 'Q96144081','Q3738699','Q5568256']\n",
    "occupation_qids_3 = [x for x in occupation_qids_3 if x not in a ]\n",
    "occupation_qids_4 = [x for x in occupation_qids_4 if x not in a ]\n",
    "occupation_qids_5 = [x for x in occupation_qids_5 if x not in a ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occupation = pd.DataFrame()\n",
    "df_occupation['occupation_qids'] = occupation_qids_5\n",
    "df_occupation['occupation_label'] = df_occupation['occupation_qids'].apply(lambda x : find_label(x))\n",
    "df_occupation.to_json('data/occupation5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json('data/occupation1.json')\n",
    "df2 = pd.read_json('data/occupation2.json')\n",
    "df3 = pd.read_json('data/occupation3.json')\n",
    "df4 = pd.read_json('data/occupation4.json')\n",
    "df5 = pd.read_json('data/occupation5.json')\n",
    "df = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
